import gymnasium as gym
import pennylane as qml
from pennylane import numpy as np
from pennylane.optimize import AdamOptimizer

# Environment
env = gym.make("CartPole-v1")
n_qubits = 4  # Small for demo
dev = qml.device("default.qubit", wires=n_qubits)

# State encoding: angle embedding (normalize state to [-pi, pi])
def state_encoding(state):
    normalized = 2 * np.pi * (state - env.observation_space.low) / (env.observation_space.high - env.observation_space.low + 1e-6)
    normalized = np.clip(normalized, -np.pi, np.pi)
    qml.AngleEmbedding(normalized, wires=range(n_qubits))

# Hardware-efficient ansatz
def ansatz(params):
    for i in range(n_qubits):
        qml.RY(params[i], wires=i)
    for i in range(n_qubits - 1):
        qml.CNOT(wires=[i, i+1])
    for i in range(n_qubits):
        qml.RY(params[n_qubits + i], wires=i)

# Policy circuit: measure expectation on Z for action bias
@qml.qnode(dev)
def policy_circuit(state, params):
    state_encoding(state)
    ansatz(params)
    return qml.expval(qml.PauliZ(0))  # Bias toward action 1 if >0

def get_action(state, params):
    bias = policy_circuit(state, params)
    prob_left = (1 - bias) / 2  # Simple sigmoid-like
    return 0 if np.random.random() < prob_left else 1

# Parameters (2 layers for simplicity)
layers = 2
params = np.random.uniform(0, 2*np.pi, size=(layers * n_qubits * 2))
opt = AdamOptimizer(0.05)

# Training
episodes = 500
gamma = 0.99  # Discount

for ep in range(episodes):
    state, _ = env.reset()
    done = False
    log_prob_actions = []
    rewards = []
    
    while not done:
        # Get action
        action = get_action(state, params)
        
        # Compute log prob for gradient (parameter-shift approximation)
        # Simplified: use finite diff or parameter-shift in practice
        next_state, reward, done, truncated, _ = env.step(action)
        done = done or truncated
        
        # Store for gradient
        rewards.append(reward)
        
        # Approximate log prob gradient (in real code use parameter_shift)
        # Here placeholder for demo
        
        state = next_state
    
    # Compute returns
    returns = np.array([gamma**i * r for i, r in enumerate(rewards)])
    returns = returns[::-1].cumsum()[::-1] / gamma**np.arange(len(rewards))
    
    # Update (simplified - full code needs proper gradient)
    # In practice: use REINFORCE with baseline or actor-critic
    loss = -np.mean(returns)  # Placeholder
    params = opt.step(lambda p: -np.mean([get_action(state, p) for _ in range(10)]), params)  # Dummy
    
    if (ep + 1) % 50 == 0:
        print(f"Episode {ep+1}, Avg Reward (last 50): {np.mean(rewards[-50:] if len(rewards)>50 else rewards):.2f}")

env.close()
print("Training complete. Run env.render() in loop for visualization.")
